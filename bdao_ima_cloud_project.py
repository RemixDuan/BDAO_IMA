# -*- coding: utf-8 -*-
"""BDAO_IMA_Cloud_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Q4a0S6J4WKdcM829EqB8xzdeBGMshte

# **Authentication**

Everytime before you make connection to Google Cloud via API, you need to run the below code to grant access to Google Cloud services.
"""

# Authenticate and access Google Cloud services
from google.colab import auth
auth.authenticate_user()

"""# **Load data into BigQuery**

The following codes help you load data from bucket to your BigQuery.

### Step 0: (Optional) Create a new project in Google Cloud

If you want an independent project to handle the data and analysis, you can create a new project in your Google Cloud.
![picture](https://drive.google.com/uc?id=18bayQbOjHw8Uq-QkMwYhupi8FPtT9Dmm)

Of course, you can continue using the existed project if you wish.

### Step 1: Create a new dataset in your BigQuery to store your project data
"""

# Import the necessary libraries
from google.cloud import storage
from google.cloud import bigquery

def create_dataset(project_id, dataset_id, location="US"):
    client = bigquery.Client(project=project_id)
    dataset_ref = bigquery.DatasetReference(project_id, dataset_id)
    dataset = bigquery.Dataset(dataset_ref)
    dataset.location = location

    created_dataset = client.create_dataset(dataset, timeout=30)  # Waits 30 seconds
    print("Dataset {} created.".format(created_dataset.dataset_id))

# Usage
create_dataset('bdao-ima01', 'restaurant_dataset') # Replace 'your-project-id' with your Google Cloud project ID and 'your-new-datatset-name' with your desired dataset name

"""### Step 2: Load data into the dataset you just created in BigQuery"""

# Load the restaurant data into BigQuery
# Replace 'your-project-id' with your Google Cloud project ID
# Replace 'your-dataset-name' with your dataset name
# (optional) you could change the table name with a new name but not a problem not to change
project_id = "bdao-ima01"
dataset_name = "restaurant_dataset"
table_name = "restaurant_data"
bucket_name = "yelp-data-bdao" # don't make changes here
blob_name = "restaurant_data.csv" # don't make changes here

bigquery_client = bigquery.Client(project=project_id)

dataset_ref = bigquery_client.dataset(dataset_name)
table_ref = dataset_ref.table(table_name)

job_config = bigquery.LoadJobConfig()
job_config.autodetect = True
job_config.source_format = bigquery.SourceFormat.CSV

load_job = bigquery_client.load_table_from_uri(
    f"gs://{bucket_name}/{blob_name}",
    table_ref,
    job_config=job_config
)

load_job.result()

print(f"Data loaded into BigQuery: {project_id}.{dataset_name}.{table_name}")

# Load the restaurant tips data into BigQuery
# Replace 'your-project-id' with your Google Cloud project ID
# Replace 'your-dataset-name' with your dataset name
# (optional) you could change the table name with a new name but not a problem not to change
project_id = "bdao-ima01"
dataset_name = "restaurant_dataset"
table_name = "restaurant_tips"
bucket_name = "yelp-data-bdao" # don't make changes here
blob_name = "restaurant_tips.csv" # don't make changes here

bigquery_client = bigquery.Client(project=project_id)

dataset_ref = bigquery_client.dataset(dataset_name)
table_ref = dataset_ref.table(table_name)

job_config = bigquery.LoadJobConfig()
job_config.autodetect = True
job_config.source_format = bigquery.SourceFormat.CSV

load_job = bigquery_client.load_table_from_uri(
    f"gs://{bucket_name}/{blob_name}",
    table_ref,
    job_config=job_config
)

load_job.result()

print(f"Data loaded into BigQuery: {project_id}.{dataset_name}.{table_name}")

# Load the data into BigQuery
# Replace 'your-project-id' with your Google Cloud project ID
# Replace 'your-dataset-name' with your dataset name
# (optional) you could change the table name with a new name but not a problem not to change
project_id = "bdao-ima01"
dataset_name = "restaurant_dataset"
table_name = "restaurant_checkin"
bucket_name = "yelp-data-bdao" # don't make changes here
blob_name = "restaurant_checkin.csv" # don't make changes here

bigquery_client = bigquery.Client(project=project_id)

dataset_ref = bigquery_client.dataset(dataset_name)
table_ref = dataset_ref.table(table_name)

job_config = bigquery.LoadJobConfig()
job_config.autodetect = True
job_config.source_format = bigquery.SourceFormat.CSV

load_job = bigquery_client.load_table_from_uri(
    f"gs://{bucket_name}/{blob_name}",
    table_ref,
    job_config=job_config
)

load_job.result()

print(f"Data loaded into BigQuery: {project_id}.{dataset_name}.{table_name}")

"""Until here, you have successfully loaded all data into your data warehouse (BigQuery) in Google Cloud.

# **Query data from BigQuery and do analysis**

Now that the data is stored in BigQuery, you can make connection to BigQuery and extract data you want.

### Query data from restaurant data file
"""

# Import the necessary libraries
from google.cloud import storage
from google.cloud import bigquery

# First make connection to BigQuery
# Replace 'your-project-id' with your Google Cloud project ID
# Replace 'your-dataset-name' with your dataset name
project_id = "bdao-ima01"
dataset_name = "restaurant_dataset"
table_name = "restaurant_data" # if you make changes previously, then here you need to make according change

bigquery_client = bigquery.Client(project=project_id)

# Query all data from the table
query = f"""
    SELECT *
    FROM `{project_id}.{dataset_name}.{table_name}`
"""

query_job = bigquery_client.query(query)
results = query_job.result()

restaurant_df = query_job.to_dataframe()
restaurant_df.head()

# Query data from selected columns from the table: Attributes
query = f"""
    SELECT business_id, stars, RestaurantsDelivery, OutdoorSeating, BusinessAcceptsCreditCards, BusinessParking, BikeParking, RestaurantsPriceRange2, RestaurantsTakeOut, ByAppointmentOnly, WiFi,
    Alcohol, Caters, RestaurantsAttire, RestaurantsReservations, Ambience, GoodForKids, CoatCheck, DogsAllowed, RestaurantsTableService, RestaurantsGoodForGroups, WheelchairAccessible, HasTV,
    HappyHour, DriveThru, NoiseLevel, GoodForMeal, BusinessAcceptsBitcoin, Smoking, Music, GoodForDancing, BestNights, BYOB, Corkage, Parking
    FROM `{project_id}.{dataset_name}.{table_name}`
"""

query_job = bigquery_client.query(query)
results = query_job.result()

restaurant_attributes_df = query_job.to_dataframe()
restaurant_attributes_df.head()

"""### Query data from restaurant checkin data file"""

# Import the necessary libraries
from google.cloud import storage
from google.cloud import bigquery

# First make connection to BigQuery
# Replace 'your-project-id' with your Google Cloud project ID
# Replace 'your-dataset-name' with your dataset name
project_id = "bdao-ima01"
dataset_name = "restaurant_dataset"
table_name = "restaurant_checkin" # if you make changes previously, then here you need to make according change

bigquery_client = bigquery.Client(project=project_id)

# Query all data from the table
query = f"""
    SELECT *
    FROM `{project_id}.{dataset_name}.{table_name}`
"""

query_job = bigquery_client.query(query)
results = query_job.result()

checkin_df = query_job.to_dataframe()
checkin_df.head()

# Query data based on condition from the table
query = f"""
    SELECT *
    FROM `{project_id}.{dataset_name}.{table_name}`
    WHERE year >=2020
"""

query_job = bigquery_client.query(query)
results = query_job.result()

checkin_after2020_df = query_job.to_dataframe()
checkin_after2020_df.head()

"""### Query data from restaurant tips data file"""

# Import the necessary libraries
from google.cloud import storage
from google.cloud import bigquery

# First make connection to BigQuery
# Replace 'your-project-id' with your Google Cloud project ID
# Replace 'your-dataset-name' with your dataset name
project_id = "bdao-ima01"
dataset_name = "restaurant_dataset"
table_name = "restaurant_tips" # if you make changes previously, then here you need to make according change

bigquery_client = bigquery.Client(project=project_id)

# Query all data from the table
query = f"""
    SELECT *
    FROM `{project_id}.{dataset_name}.{table_name}`
"""

query_job = bigquery_client.query(query)
results = query_job.result()

tips_df = query_job.to_dataframe()
tips_df

"""### Joining tables through SQL query"""

# Import the necessary libraries
from google.cloud import storage
from google.cloud import bigquery

# First make connection to BigQuery
# Replace 'your-project-id' with your Google Cloud project ID
# Replace 'your-dataset-name' with your dataset name
project_id = "bdao-ima01"
dataset_name = "restaurant_dataset"
table_1 = "restaurant_data"
table_2 = "restaurant_checkin"
table_3 = "restaurant_tips"

bigquery_client = bigquery.Client(project=project_id)

# Adding restaurant information to checkin data by joining 'restaurant_checkin' and 'restaurant_data'
query = f"""
    SELECT r.business_id, r.name, r.city, r.state, r.stars, r.review_count, r.is_open, r.categories,c.business_id, c.total_number_of_checkin
    FROM `{project_id}.{dataset_name}.{table_2}` AS c
    JOIN `{project_id}.{dataset_name}.{table_1}` AS r on r.business_id = c.business_id
"""

query_job = bigquery_client.query(query)
results = query_job.result()

join_df = query_job.to_dataframe()
join_df.head()

"""When joining, be careful of one-to-many. In restaurant data there are information for each restaurant, but in checkin and tips data there are multiple records for one single restaurants. Thus, it is better to use checkin or tips data to join restaurant data. If you do the opposite, you might have error and query incomplete data.

If you are just not familiar with SQL, you can just query dataset one by one and then use Python to merge or join them if you need.

# **Data processing**

Since the query data is converted into dataframe, you can easily use Python to do analysis or modelling. This is up to you how you would do.

##Restaurant_data processing

###Create a df without attributes
"""

selected_columns = [
    'attributes', 'RestaurantsDelivery', 'OutdoorSeating', 'BusinessAcceptsCreditCards', 'BusinessParking',
    'BikeParking', 'RestaurantsPriceRange2', 'RestaurantsTakeOut', 'ByAppointmentOnly', 'WiFi',
    'Alcohol', 'Caters', 'RestaurantsAttire', 'RestaurantsReservations', 'Ambience', 'GoodForKids',
    'CoatCheck', 'DogsAllowed', 'RestaurantsTableService', 'RestaurantsGoodForGroups', 'WheelchairAccessible', 'HasTV',
    'HappyHour', 'DriveThru', 'NoiseLevel', 'GoodForMeal', 'BusinessAcceptsBitcoin', 'Smoking',
    'Music', 'GoodForDancing', 'BestNights', 'BYOB', 'Corkage', 'Parking'
]

# Create a dataframe without attributes
restaurant_withoutattributes_df = restaurant_df.drop(columns=selected_columns)
restaurant_withoutattributes_df.head()

restaurant_withoutattributes_df.shape

"""###Check missing value"""

# Calculate the number of missing values in each column
missing_values = restaurant_withoutattributes_df.isnull().sum()

columns_with_missing = missing_values[missing_values > 0] # show missing value column > 0
print(columns_with_missing)

restaurant_withoutattributes_df.drop(columns=['address', 'postal_code', 'hours'], inplace=True)
restaurant_withoutattributes_df.head()

"""###Check duplicates"""

# Number of unique and duplicated rows
restaurant_withoutattributes_df.duplicated().value_counts()

"""Download Restaurant without attributes data"""

# restaurant_withoutattributes_df.to_csv('/content/Restaurant without attributes data.csv', index=False)

"""##Restaurant_checkin processing"""

checkin_df.head()

# Find the number of rows and columns
checkin_df.shape

"""###Check missing value"""

checkin_df.isnull().sum()

# sort the table from high to low
# checkin_df = checkin_df.sort_values(by='total_number_of_checkin')
# checkin_df

"""###Convert to year-month data"""

# Delete irrelevant columns
checkin_yearmonth_df = checkin_df.drop(columns=['total_number_of_checkin', 'restaurant', 'day', 'hour'])

# Extract the year and month and create a new column
checkin_yearmonth_df['date'] = checkin_yearmonth_df['date'].dt.to_period('M')
checkin_yearmonth_df.head()

"""###Download Checkin year-month data"""

# checkin_yearmonth_df.to_csv('/content/Checkin year-month data.csv', index=False)

"""##Restaurant_tips processing"""

tips_df.head()

tips_df.shape

"""###Check missing value"""

tips_df.isnull().sum()

"""###Check duplicates"""

# Number of unique and duplicated rows
tips_df.duplicated().value_counts()

# Check and show duplicated rows
duplicate_rows = tips_df.duplicated(keep=False)  # keep=False will mark all duplicate lines
tips_duplicates = tips_df[duplicate_rows]
tips_duplicates

# Delete duplicates
tips_df = tips_df.drop_duplicates()
tips_df = tips_df.reset_index(drop=True)
tips_df

"""##Join_df processing"""

join_df.head()

join_df.shape

"""###Check missing value"""

join_df.isnull().sum()

"""###Check duplicates"""

# Number of unique and duplicated rows
join_df.duplicated().value_counts()

# Delete duplicates
join_df = join_df.drop_duplicates()
join_df = join_df.reset_index(drop=True)

# Delete 'business_id_1' for irrelevant
join_df.drop(['business_id_1'], axis=1, inplace=True)
join_df.head()

"""###Download Restaurant-checkin join data"""

# join_df.to_csv('/content/Restaurant-Checkin join data.csv', index=False)

"""#**Data preparation**"""

checkin_after2020_df.head()

"""##Calculate each business total checkins after 2020"""

# Group by business_id and count the number of checkins after 2020 for each business_id
checkin_after2020_total_df = checkin_after2020_df.groupby('business_id').size().reset_index(name='total_checkins_after_2020')
checkin_after2020_total_df

"""##Filter restaurants with stars greater than 4"""

# Filter restaurants with stars > 4
restaurant_stars4_df = restaurant_withoutattributes_df[restaurant_withoutattributes_df['stars'] >= 4]
restaurant_stars4_df = restaurant_stars4_df.reset_index(drop=True)
restaurant_stars4_df.head()

restaurant_stars4_df.shape

"""##Merge Tables of total checkins after 2020 and join_in df"""

import pandas as pd
restaurant_checkin_after2020_total_df = pd.merge(checkin_after2020_total_df, join_df, on='business_id', how='inner')
restaurant_checkin_after2020_total_df.head()

restaurant_checkin_after2020_total_df.shape

# Delete duplicates
restaurant_checkin_after2020_total_df = restaurant_checkin_after2020_total_df.drop_duplicates()
restaurant_checkin_after2020_total_df = restaurant_checkin_after2020_total_df.reset_index(drop=True)

"""###Download 'Checkin data after 2020'"""

# restaurant_checkin_after2020_total_df.to_csv('/content/Checkin data after 2020.csv', index=False)

"""##Merge Tables of total checkins after 2020 and restaurant stars > 4"""

restaurant_checkin_star4_after2020_df = pd.merge(checkin_after2020_total_df, restaurant_stars4_df, on='business_id', how='inner')
restaurant_checkin_star4_after2020_df.head()

"""##Choose Top1000 total checkins restaurants after 2020"""

# Sort by 'total_checkins_after_2020' column in descending order
restaurant_checkin_top1000_df = restaurant_checkin_star4_after2020_df.sort_values(by='total_checkins_after_2020', ascending=False).head(1000)
restaurant_checkin_top1000_df

"""###Download 'Restaurant data top 1000 after 2020'"""

# restaurant_checkin_top1000_df.to_csv('/content/Restaurant data top 1000 after 2020.csv', index=False)

"""#**Feature Engineering for Attributes**"""

restaurant_attributes_df.head()

"""##Download 'Restaurant attributes data'"""

# restaurant_attributes_df.to_csv('/content/Restaurant attributes data.csv', index=False)

# Number of the missing data
missing_values = restaurant_attributes_df.isnull().sum()
print(missing_values)

# Filter out columns with missing values greater than 25,000
columns_with_many_missing = missing_values[missing_values > 25000]
print(columns_with_many_missing)

# Delete irrelevant columns
restaurant_attributes_df.drop(["BusinessParking", "Ambience", "GoodForMeal", "RestaurantsAttire"], axis=1, inplace=True)

# List current columns after deleting
restaurant_attributes_df.columns

# Delete columns with missing values greater than 25,000
restaurant_attributes_df.drop(columns=['ByAppointmentOnly', 'CoatCheck', 'DogsAllowed', 'RestaurantsTableService',
    'WheelchairAccessible', 'HappyHour', 'DriveThru', 'BusinessAcceptsBitcoin',
    'Smoking', 'Music', 'GoodForDancing', 'BestNights', 'BYOB', 'Corkage'], inplace=True)

restaurant_attributes_df.head()

# Number of the missing data
missing_values = restaurant_attributes_df.isnull().sum()
print(missing_values)

# Delete the missing data
restaurant_attributes_df = restaurant_attributes_df.dropna()
restaurant_attributes_df = restaurant_attributes_df.reset_index(drop=True)
restaurant_attributes_df

# Convert Boolean values to integers
restaurant_attributes_df = restaurant_attributes_df.applymap(lambda x: 1 if x is True else (0 if x is False else x))
restaurant_attributes_df.head()

# Make one-hot encoding and convert to integer type
wifi_dummies = pd.get_dummies(restaurant_attributes_df['WiFi'], prefix='WiFi')
wifi_dummies = wifi_dummies.astype(int)

# Delete oringinal column
restaurant_attributes_df.drop('WiFi', axis=1, inplace=True)
restaurant_attributes_df = pd.concat([restaurant_attributes_df, wifi_dummies], axis=1)
restaurant_attributes_df.head()

# Make one-hot encoding and convert to integer type
noise_level_dummies = pd.get_dummies(restaurant_attributes_df['NoiseLevel'], prefix='NoiseLevel')
noise_level_dummies = noise_level_dummies.astype(int)

# Delete oringinal column
restaurant_attributes_df.drop('NoiseLevel', axis=1, inplace=True)
restaurant_attributes_df = pd.concat([restaurant_attributes_df, noise_level_dummies], axis=1)
restaurant_attributes_df.head()

# Make one-hot encoding and convert to integer type
alcohol_dummies = pd.get_dummies(restaurant_attributes_df['Alcohol'], prefix='Alcohol')
alcohol_dummies = alcohol_dummies.astype(int)

# Delete oringinal column
restaurant_attributes_df.drop('Alcohol', axis=1, inplace=True)
restaurant_attributes_df = pd.concat([restaurant_attributes_df, alcohol_dummies], axis=1)
restaurant_attributes_df.head()

restaurant_attributes_heatmap_df = restaurant_attributes_df.drop(columns='business_id')

import seaborn as sns
import matplotlib.pyplot as plt

# Create a correlation matrix rounding to one decimal point
correlation_matrix = restaurant_attributes_heatmap_df.corr().round(2)

# Enlarge the image for better viewing
plt.figure(figsize=(14, 12))

# Print a correlation heat map
sns.heatmap(data=correlation_matrix, cmap='coolwarm', annot=True)

"""#**Data Analysis**

Since we have selected the cities for restaurant expansion, here we will statistically analyse the checkins for each city.
"""

# Group by city and calculate total check-ins
city_checkins_total = restaurant_checkin_after2020_total_df.groupby('city')['total_checkins_after_2020'].sum()

# Sort cities based on total check-ins in descending order and select top 10
top_8_cities = city_checkins_total.sort_values(ascending=False).head(8)
top_8_cities

# Filter the original DataFrame to include only data for the top 10 cities
top_8_cities_df = restaurant_checkin_after2020_total_df[restaurant_checkin_after2020_total_df['city'].isin(top_8_cities.index)]

# Calculate summary statistics for the top 8 cities
top_8_cities_df.groupby('city')['total_checkins_after_2020'].describe()

import seaborn as sns
import matplotlib.pyplot as plt
# Create subplots for each column
top_8_cities_df.hist(figsize=(10, 8))

# Adjust the layout and display the plot
plt.tight_layout()
plt.show()

sns.boxplot(x='city', y='total_checkins_after_2020', data=top_8_cities_df)
plt.title('Boxplot of Checkins for Top 8 Cities')
plt.xlabel('City')
plt.ylabel('Check-ins')
plt.xticks(rotation=45)

"""# **Topic Modelling for review analysis**

Latent Dirichlet Allocation (LDA) is a classic model to do topic modelling. Topic modeling is unsupervised learning and the goal is to group different documents to the same “topic”.
"""

import pandas as pd

"""##**1. Merge Tables**"""

restaurant_checkin_top1000_tips_df = pd.merge(restaurant_checkin_top1000_df, tips_df, on='business_id', how='inner')

# Delete duplicates
df = restaurant_checkin_top1000_tips_df.drop_duplicates()
df = df.reset_index(drop=True)
df.head()

df.shape

target_column = 'text' #change the column name to where the review is

"""## **2. Text cleaning & Text processing**
Before you do topic modeling, you need make sure you clean and process the text.
"""

!pip install contractions
import re
import string
import contractions
import nltk
import gensim
from gensim.utils import simple_preprocess
import spacy
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# drop data with missing values in the 'content' column
df = df.dropna(subset=[target_column])

# drop duplicate review content
df = df.drop_duplicates(subset=[target_column])

# remove contraction
df[target_column] = df[target_column].map(lambda x: contractions.fix(x))

# convert the relevant column to lowercase
df[target_column] = df[target_column].str.lower()

# Remove overspace
df[target_column] = df[target_column].map(lambda x: re.sub('\s{2,}', " ", x))

# Remove non-word characters, so numbers and ___ etc
df[target_column] = df[target_column].str.replace("[^A-Za-z]", " ", regex = True)

# Remove punctuation
df[target_column] = df[target_column].map(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))

# create a list of the contents from the 'contents' column
words = df[target_column].tolist()

# tokenise the words
word_tokens = []
for content in words:
    word_tokens.append(word_tokenize(content))

# create bigram model
bigram = gensim.models.phrases.Phrases(word_tokens, min_count=3, threshold=10)
bigram_mod = gensim.models.phrases.Phraser(bigram) # Faster way to get a sentence clubbed as a trigram/bigram

# NLTK Stop words
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['flight']) #add more stopwords here

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python3 -m spacy download en
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# Define functions for stopwords, bigrams and lemmatisation
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

# Remove stopwords
data_words_nostops = remove_stopwords(word_tokens)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# Do lemmatisation keeping only noun, adj, vb, adv
data_lemmatised = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

# put the tokens back together as text to have our filtered contents

rejoin = []
for content in data_lemmatised: # Here we choose to use stemming instead of lemmatisation
    x = " ".join(content) # join the text back together
    rejoin.append(x)

# add the reformed text to the data frame
df['cleaned_review'] = rejoin

df.head()

"""## **3. Build the LDA model**"""

# vectorise the data into word counts

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA

max_words = 1000 #how many words taking account for topic modeling
vectorizer = CountVectorizer(max_features=max_words)
vec = vectorizer.fit_transform(df['cleaned_review'])

k = 6 #this is the number of the topic. you can decide the number

lda = LDA(n_components=k, max_iter=5, learning_method='online', random_state = 10)
lda.fit(vec)

"""## **4. Visualisation of the topics**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import wordcloud

#declaring number of terms we need per topic
terms_count = 50

terms = vectorizer.get_feature_names_out()

wcloud = wordcloud.WordCloud(background_color="White",mask=None, max_words=100,\
                             max_font_size=60,min_font_size=10,prefer_horizontal=0.9,
                             contour_width=3,contour_color='Black',colormap='Set2')

fig, axes = plt.subplots(2, 3, figsize=(30, 15), sharex=True)
axes = axes.flatten()

for idx,topic in enumerate(lda.components_):
    print('Topic# ',idx+1)
    abs_topic = abs(topic)
    topic_terms = [[terms[i],topic[i]] for i in abs_topic.argsort()[:-terms_count-1:-1]]
    topic_terms_sorted = [[terms[i], topic[i]] for i in abs_topic.argsort()[:-terms_count - 1:-1]]
    topic_words = []
    for i in range(terms_count):
        topic_words.append(topic_terms_sorted[i][0])
    print(','.join( word for word in topic_words))
    print("")
    dict_word_frequency = {}

    for i in range(terms_count):
        dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]

    ax = axes[idx]
    ax.set_title(f'Topic {idx +1}',fontdict={'fontsize': 30})
    wcloud.generate_from_frequencies(dict_word_frequency)
    ax.imshow(wcloud, interpolation='bilinear')
    ax.axis("off")

# Commented out IPython magic to ensure Python compatibility.
#

# %matplotlib inline
import matplotlib.pyplot as plt

# helper function to plot topics
# see Grisel et al.
# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html
def plot_top_words(model, feature_names, n_top_words, title):
    fig, axes = plt.subplots(1, 6, figsize=(30, 15), sharex=True)
    axes = axes.flatten()
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
        top_features = [feature_names[i] for i in top_features_ind]
        weights = topic[top_features_ind]

        ax = axes[topic_idx]
        ax.barh(top_features, weights, height=0.7)
        ax.set_title(f'Topic {topic_idx +1}',
                     fontdict={'fontsize': 30})
        ax.invert_yaxis()
        ax.tick_params(axis='both', which='major', labelsize=20)
        for i in 'top right left'.split():
            ax.spines[i].set_visible(False)
        fig.suptitle(title, fontsize=40)

    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)
    plt.show()

n_top_words = 20  #how many words to be visualised in each topic

# get the list of words (feature names)
vec_feature_names = vectorizer.get_feature_names_out()

# print the top words per topic
plot_top_words(lda, vec_feature_names, n_top_words, 'Topics in LDA model')